#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jul  6 15:22:31 2023

@author: bianogueira
"""

from bs4 import BeautifulSoup as bs
import time
import pandas as pd
# import requests
# import numpy as np
import datetime
import webbrowser
import traceback
import os
import re
# import subprocess as sp
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium import webdriver
# from selenium.common.exceptions import WebDriverException
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
# from selenium.common.exceptions import ElementNotInteractableException
from selenium.common.exceptions import ElementClickInterceptedException
# from IPython.core.display import display, HTML


from dash import Dash, dcc, html, dash_table



######################################## functions ######################################## 

def to_clickable_html(df, file_path, header):
    html_df = dfnoticias.copy()
    # Convert the link column to clickable links
    html_df['Link'] = html_df['Link'].apply(lambda x: f'<a href="{x}">{x}</a>')
    # Convert the DataFrame to HTML table
    html = html_df.to_html(index=False, escape=False,classes='table table-stripped')
    html = header + html
    #write to a local file
    with open(file_path, "w", encoding="utf-8") as file:
        file.writelines('<meta charset="UTF-8">\n')
        file.write(html)
    return html


def scrape_valor(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(1)
    try:
        WebDriverWait(driver, 1.5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="onesignal-slidedown-cancel-button"]'))).click()
    except TimeoutException:
        pass
    try:
        WebDriverWait(driver, 1.5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="gcomPromo"]/div/div[1]/span/span'))).click()
    except TimeoutException:
        pass
    
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
    time.sleep(3)
    
    soup = bs(driver.page_source, "html.parser")    
    noticiasel = [item for item in soup.find_all('div', attrs = {"class":"bastian-feed-item"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            if len(noticiasel[i].find_all('span', attrs = {"class":"feed-post-datetime"})) == 0:
                print(noticiasel[i].text)
                continue
            infodata = noticiasel[i].find_all('span', attrs = {"class":"feed-post-datetime"})[0].text
            if "Há" in infodata:
                dicNoticia['Data'] = agora.strftime("%d/%m/%Y")
                if "minutos" in infodata:
                    dicNoticia['Hora'] = (agora - datetime.timedelta(minutes = int(re.findall(r'\d+',infodata)[0]))) .strftime("%H:%M")
                elif "horas" in infodata:
                    horanoticia = (agora - datetime.timedelta(hours = int(re.findall(r'\d+',infodata)[0]))) .strftime("%H:%M")
                    dicNoticia['Hora'] = horanoticia
                    if horanoticia > agora.strftime("%H:%M"):
                        dicNoticia['Data'] = (agora - datetime.timedelta(days = 1)).strftime("%d/%m/%Y")
                    
                else:
                    continue
            elif infodata == 'Agora':
                dicNoticia['Data'] = agora.strftime("%d/%m/%Y")
                dicNoticia['Hora'] = agora.strftime("%H:%M")
            elif "Ontem" in infodata:
                continue
            else: 
                dicNoticia['Data'] =  noticiasel[i].find_all('span', attrs = {"class":"feed-post-datetime"})[0].text.split(', ')[0]
                dicNoticia['Hora'] = noticiasel[i].find_all('span', attrs = {"class":"feed-post-datetime"})[0].text.split(', ')[1]
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('div', attrs = {"class":"feed-post-body-title"})[0].text
            dicNoticia['Link'] = noticiasel[i].find_all('a', attrs = {"class":"feed-post-link"})[0].get('href')
            noticias.append(dicNoticia)
        else:
            
            continue
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])< agora.strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop


def scrape_estadao(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(1)
    
    try:
        
        time.sleep(2)
        try: 
            driver.find_elements(By.CSS_SELECTOR, value = "button[class = 'lgpd-button']")[0].click()
        except IndexError:
            pass
        driver.find_elements(By.CSS_SELECTOR, value = "input[id = 'email_login']")[0].click()
        driver.find_elements(By.CSS_SELECTOR, value = "input[id = 'email_login']")[0].send_keys('macro@bnymellon.com.br')
        driver.find_elements(By.CSS_SELECTOR, value = "input[id = 'senha']")[0].click()
        driver.find_elements(By.CSS_SELECTOR, value = "input[id = 'senha']")[0].send_keys('arx633401')
        driver.execute_script("window.scrollTo(0,50);")
        driver.find_elements(By.CSS_SELECTOR, value = "input[id = 'btn-login']")[0].click()
    except IndexError:
        pass
    
    
    time.sleep(3.5)
    
    
    try:
    
        botao = driver.find_element(By.XPATH,"//button[contains(.,'Carregar mais notícias')]")
        driver.execute_script("arguments[0].scrollIntoView();",botao)
        driver.execute_script("window.scrollTo(0,window.scrollY - 200)")
        
        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[contains(.,'Carregar mais notícias')]"))).click()
        time.sleep(1)
    except NoSuchElementException:
        pass
    
    
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find_all('div', attrs = {"class":"noticias-mais-recenter--item"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] =  noticiasel[i].find_all('span', attrs = {"class":"date"})[0].text.split(' | ')[0]
            dicNoticia['Hora'] = noticiasel[i].find_all('span', attrs = {"class":"date"})[0].text.split(' | ')[1].replace("h",":")
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h3', attrs = {"class":"headline"})[0].text
            dicNoticia['Link'] = noticiasel[i].find_all('div', attrs = {"class":"content"})[0].find_all('a')[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    

    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop


def scrape_folha(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(3)
    
    dicMeses = {'jan':'01',
     'fev':'02',
     'mar':'03',
     'abr':'04',
     'mai':'05',
     'jun':'06',
     'jul':'07',
     'ago':'08',
     'set':'09',
     'out':'10',
     'nov':'11',
     'dez':'12',
    }
    
    
    try:
        WebDriverWait(driver, 7).until(EC.element_to_be_clickable((By.XPATH, "button[class = 'banner-lgpd-consent__accept']"))).click()
    except TimeoutException:
        pass
        
    botao = driver.find_element(By.XPATH, "//button[contains(.,'Ver mais')]")
    driver.execute_script("arguments[0].scrollIntoView();",botao)
    driver.execute_script("window.scrollTo(0,window.scrollY - 200)")
    # driver.execute_script("window.scrollTo(0,3000);")
    time.sleep(2)
    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[contains(.,'Ver mais')]"))).click()
    time.sleep(3)
    soup = bs(driver.page_source, "html.parser")
    noticiasel = [item for item in soup.find_all('li', attrs = {"class":"c-main-headline"})]
    noticiasel = noticiasel + [item for item in soup.find_all('li', attrs = {"class":"c-headline--newslist"})]
    noticias = []
    for i in range(len(noticiasel)):
        if i == 0:
            if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
                dicNoticia = dict()
                infodata = noticiasel[i].find_all('time', attrs = {"class":"c-headline__dateline"})[0].text.split(' às ')[0].replace("\n                  ","").replace(".","/")
                dia = infodata[-len(infodata):-8]
                if(len(dia)==2):dia= "0"+dia
                dicNoticia['Data'] = dia + str(dicMeses[infodata[-8:-5]]) + infodata[-5:]
                dicNoticia['Hora'] = noticiasel[i].find_all('time', attrs = {"class":"c-headline__dateline"})[0].text.split(' às ')[1].replace("h",":").replace("\n            ","")
                dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
                dicNoticia['Titulo'] = noticiasel[i].find_all('h2', attrs = {"class":"c-main-headline__title"})[0].text
                dicNoticia['Link'] = noticiasel[i].find_all('a', attrs = {"class":"c-main-headline__url"})[0].get('href')
                noticias.append(dicNoticia)
            else:
                continue
        else: 
            if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
                dicNoticia = dict()
                infodata = noticiasel[i].find_all('time', attrs = {"class":"c-headline__dateline"})[0].text.split(' às ')[0].replace("\n                      ","").replace(".","/")
                dia = infodata[-len(infodata):-8]
                if(len(dia)==2):dia= "0"+dia
                dicNoticia['Data'] = dia + str(dicMeses[infodata[-8:-5]]) + infodata[-5:]
                dicNoticia['Hora'] = noticiasel[i].find_all('time', attrs = {"class":"c-headline__dateline"})[0].text.split(' às ')[1].replace("h",":").replace("\n                ","")
                dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
                dicNoticia['Titulo'] = noticiasel[i].find_all('h2', attrs = {"class":"c-headline__title"})[0].text
                dicNoticia['Link'] = noticiasel[i].find_all('div', attrs = {"class":"c-headline__content"})[0].find_all('a')[0].get('href')
                noticias.append(dicNoticia)
            else:
                continue
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop



def scrape_cnnbr(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(1)

    
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
    time.sleep(5)
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find_all('li', attrs = {"class":"home__list__item"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] =  noticiasel[i].find_all('span', attrs = {"class":"home__title__date"})[0].text.split(' às ')[0].replace(" ", "")
            dicNoticia['Hora'] = noticiasel[i].find_all('span', attrs = {"class":"home__title__date"})[0].text.split(' às ')[1]
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h3', attrs = {"class":"news-item-header__title market__new__title"})[0].text
            dicNoticia['Link'] = noticiasel[i].find_all('a', attrs = {"class":"home__list__tag"})[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop


def scrape_poder360(driver, url, palavras_chave, agora):
    driver.get(url+agora.strftime("%Y/%m/%d"))

    
    time.sleep(3)
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find_all('div', attrs = {"class":"archive-list__text"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] =  agora.strftime("%d/%m/%Y")
            dicNoticia['Hora'] = noticiasel[i].find_all('span', attrs = {"class":"archive-list__date"})[0].text.split(' - ')[1].replace("h",":")
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h2', attrs = {"class":"archive-list__title-2 fw-light"})[0].find_all('a')[0].text
            dicNoticia['Link'] = noticiasel[i].find_all('h2', attrs = {"class":"archive-list__title-2 fw-light"})[0].find_all('a')[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    

    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
    return dfloop



def scrape_metropoles(driver, url, palavras_chave, agora):
    driver.get(url)
    
    time.sleep(1)
    try: 
        WebDriverWait(driver, 1.5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="onesignal-slidedown-cancel-button"]'))).click()
    except TimeoutException:
        pass
    try: 
        WebDriverWait(driver, 1.5).until(EC.element_to_be_clickable((By.XPATH, '//*[@class="adopt-c-bwASOE"]'))).click()
    except TimeoutException:
        pass
    
    botao = driver.find_element(By.XPATH, "//button[contains(.,'Ver mais')]")
    driver.execute_script("arguments[0].scrollIntoView();",botao)
    # driver.execute_script("window.scrollTo(0,3000);")
    time.sleep(2)
    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[contains(.,'Ver mais')]"))).click()

    time.sleep(3)
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find_all('article')]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            
            htmlantigo = bs(open('/Users/bianogueira/Documents/Documentos - MacBook Air de Beatriz/RIO/academico/puc rio economia/ARX/webscraping/newsscraping/index.html','r'), 'html.parser')
            link = noticiasel[i].find_all('h5', attrs = {"class":"noticia__titulo"})[0].find_all('a')[0].get('href')
            
            if link in str(htmlantigo):
                continue
            
            dicNoticia = dict()
            dicNoticia['Data'] =  agora.strftime("%d/%m/%Y")
            dicNoticia['Hora'] = "00:00"
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h5', attrs = {"class":"noticia__titulo"})[0].find_all('a')[0].text
            dicNoticia['Link'] = link
            
            
            
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    return dfloop



def scrape_veja(driver, url, palavras_chave, agora):
    
    dicMeses = {'jan':'01',
     'fev':'02',
     'mar':'03',
     'abr':'04',
     'mai':'05',
     'jun':'06',
     'jul':'07',
     'ago':'08',
     'set':'09',
     'out':'10',
     'nov':'11',
     'dez':'12',
    }
    
    
    
    driver.get(url)
    time.sleep(3)
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
    time.sleep(3)
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find_all('div', attrs = {"id":"infinite-list"})[0].find_all('div', attrs = {"class":"row"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            
            if len(noticiasel[i].find_all('span', attrs = {"class":"author"})[0].find_all('time')) == 0:
                infodata = noticiasel[i].find_all('span', attrs = {"class":"author"})[0].text.split("h")
                dia = infodata[0][-15:-13].replace(" ", "0")
                dicNoticia['Data'] = dia +"/" + str(dicMeses[infodata[0][-12:-9]]) +"/" + infodata[0][-8:-4]
                dicNoticia['Hora'] = infodata[0][-2:] + ":" + infodata[1][:2]
            else:
                infodata = noticiasel[i].find_all('span', attrs = {"class":"author"})[0].find_all('time')[0].text.split(', ')[0].replace("| ", "").replace(" ", "/")
                dia = infodata[-len(infodata):-8]
                if(len(dia)==2):dia= "0" +dia
                dicNoticia['Data'] = dia + str(dicMeses[infodata[-8:-5]]) + infodata[-5:]
                dicNoticia['Hora'] = noticiasel[i].find_all('span', attrs = {"class":"author"})[0].find_all('time')[0].get('datetime')
                
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h2', attrs = {"class":"title"})[0].text
            dicNoticia['Link'] = noticiasel[i].find_all("a")[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop


def scrape_aneel(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(1)
    
    driver.find_elements(By.XPATH, value = "//body")[0].click()
    time.sleep(3)
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find("ul", attrs= {"class":"noticias listagem-noticias-com-foto"}).find_all('li')]
    noticias = []
    for i in range(len(noticiasel)):
        if noticiasel[i].text != '  ':
            dicNoticia = dict()
            dicNoticia['Data'] =  noticiasel[i].find_all('span', attrs = {"class":"data"})[0].text.split('-')[0].replace(" ", "").replace("\n", "")
            dicNoticia['Hora'] = "01:00"
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h2', attrs = {"class":"titulo"})[0].text.replace("\n", "")
            dicNoticia['Link'] =  noticiasel[i].find_all('h2', attrs = {"class":"titulo"})[0].find('a').get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop


def scrape_nAgro(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(1)

    soup = bs(driver.page_source, "html.parser")
    
    
    noticiasel = [item for item in soup.find_all('li', attrs = {"class":"horizontal com-hora"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] = noticiasel[i].find_previous("h3").text
            dicNoticia['Hora'] = noticiasel[i].find_all('span', attrs = {"class":"hora"})[0].text
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h2')[0].text
            dicNoticia['Link'] =  noticiasel[i].find('a').get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop



def scrape_antagonista(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(1)

    
    botao = driver.find_element(By.XPATH, "//span[contains(.,'VER MAIS')]")
    driver.execute_script("arguments[0].scrollIntoView();",botao)
    driver.execute_script("window.scrollTo(0,window.scrollY - 10)")
    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//span[contains(.,'VER MAIS')]"))).click()
    
    time.sleep(5)
    soup = bs(driver.page_source, "html.parser")
    
    noticiasel = [item for item in soup.find_all('div', attrs = {"id":"cardImgLeftBlock"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            infodata =  noticiasel[i].find_all('div', attrs = {"class":"card-category-date"})[0].find_all("time")[0].text.split(' ')[0].replace(" ", "")
            dicNoticia['Data'] =  infodata[:-2] + '20'+ infodata[-2:]
            dicNoticia['Hora'] = noticiasel[i].find_all('div', attrs = {"class":"card-category-date"})[0].find_all("time")[0].text.split(' ')[1].replace("\n", "")
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('a', attrs = {"class":"card-title-description"})[0].text.replace("\n", "")
            dicNoticia['Link'] = noticiasel[i].find_all('a', attrs = {"class":"card-title-description"})[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop




def scrape_cepea(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(4)

    soup = bs(driver.page_source, "html.parser")
    listagem = soup.find_all("div", attrs ={'class':'imagenet-ma-t imagenet-fl imagenet-col-12'})[0]
    noticiasel = [item for item in listagem.find_all('div')]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            infodata = noticiasel[i].find_all('h4')[0].text.split(' - ')[0].replace(" ", "").replace("\n", "")
            dicNoticia['Data'] = infodata
            dicNoticia['Hora'] = "00:00"
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h4')[0].find_all('a')[0].text.replace("\n", "")
            dicNoticia['Link'] = noticiasel[i].find_all('h4')[0].find_all('a')[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop



def scrape_scot(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(5)
    

    
    soup = bs(driver.page_source, "html.parser")
    primeira = soup.find_all("div", attrs ={'class':'display_canal1'})[0].find_all("div")[0]
    noticiasel = [item for item in primeira.find_all('ul')]
    listagem_outras_titulo = soup.find_all("ul", attrs ={'class':'ulm181'})[0].select("li[class^=canal_posicao]")
    listagem_outras_hora = soup.find_all("ul", attrs ={'class':'ulm181'})[0].select("li[class^=data_canal]")
    noticiasel = noticiasel + listagem_outras_titulo
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            if i == 0:
                info = noticiasel[i].select("p[class^=arruma]")[0]
                infodata = noticiasel[i].find_all('li', attrs = {'id':'align_canal1'})[0].text.split("h")
            else:
                info = noticiasel[i].select("h2[class=titulo-arial]")[0]
                infodata = listagem_outras_hora[i-1].text.split("h")
                if not ("m" in infodata or "h" in infodata):
                    continue
            horanoticia = (agora - datetime.timedelta(hours = int(infodata[0]),minutes = int(infodata[1].replace("m", "")))) .strftime("%H:%M")
            dicNoticia['Hora'] = horanoticia
            if horanoticia > agora.strftime("%H:%M"):
                dicNoticia['Data'] = (agora - datetime.timedelta(days = 1)).strftime("%d/%m/%Y")
            else:
                dicNoticia['Data'] = agora.strftime("%d/%m/%Y")
            dicNoticia['Hora'] = (agora - datetime.timedelta(hours = int(infodata[0]),minutes = int(infodata[1].replace("m", "")))) .strftime("%H:%M")
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = info.text.replace("\n", "")
            dicNoticia['Link'] = info.find_all('a')[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop



def scrape_ce(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(4)
    
    dicMeses = {'janeiro':'01',
     'fevereiro':'02',
     'março':'03',
     'abril':'04',
     'maio':'05',
     'junho':'06',
     'julho':'07',
     'agosto':'08',
     'setembro':'09',
     'outubro':'10',
     'novembro':'11',
     'dezembro':'12',
    }
    
    

    soup = bs(driver.page_source, "html.parser")
    noticiasel = []
    noticiasel = noticiasel +[item for item in soup.select("div[class='mt-8']")[0].select("div[class^='col-span-12 md:col-span']")]
    noticiasel = noticiasel +[item for item in soup.select("div[class='mt-12']")[0].find_all('li', attrs = {'class':'bg-white overflow-hidden relative rounded'})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            info = noticiasel[i].select("a[href^='https']")[0]
            infodata = info.find_all('span')[1].text
            dicNoticia['Data'] = infodata[:2] + "/"+ dicMeses[infodata[6:-8]] + "/"+ infodata[-4:]
            dicNoticia['Hora'] =  "00:00"
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = info.find_all('p')[0].text.replace("\n", "")
            dicNoticia['Link'] = info.get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop



def scrape_agrolink(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(4)
    
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
    time.sleep(3)

    soup = bs(driver.page_source, "html.parser")
    noticiasel = [item for item in soup.find_all('div', attrs = {"class":"entry col-12 pb-0 mb-0"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] = noticiasel[i].find_all('div', {'class':'entry-meta'})[0].find_all("li")[1].text.replace("\n", "").split(" ")[0]
            dicNoticia['Hora'] =  noticiasel[i].find_all('div', {'class':'entry-meta'})[0].find_all("li")[1].text.replace("\n", "").split(" ")[1][:-3]
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h3')[0].text
            dicNoticia['Link'] = 'https://www.agrolink.com.br' +noticiasel[i].find_all('h3')[0].find('a').get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop




def scrape_crural(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(4)
    
    
    soup = bs(driver.page_source, "html.parser")
    noticiasel = [item for item in soup.find_all('div', attrs = {"class":"info"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] = noticiasel[i].find_all('div', {'class':'data-hora'})[0].text.replace("\n", "").split("às")[0].replace(" ", "")
            dicNoticia['Hora'] =  noticiasel[i].find_all('div', {'class':'data-hora'})[0].text.replace("\n", "").split("às")[1].replace(" ", "").replace("h", ":")
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('h2', attrs = {"class":"fl-post-title"})[0].text
            dicNoticia['Link'] = noticiasel[i].find_all('h2', attrs = {"class":"fl-post-title"})[0].find("a").get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop


def scrape_dtransporte(driver, url, palavras_chave, agora):
    driver.get(url)
    time.sleep(4)
    
    
    soup = bs(driver.page_source, "html.parser")
    noticiasel = [item for item in soup.find_all('div', attrs = {"class":"col s12 m6"})]
    noticias = []
    for i in range(len(noticiasel)):
        if (noticiasel[i].text != '  ' and any(palavra.lower() in noticiasel[i].text for palavra in palavras_chave)):
            dicNoticia = dict()
            dicNoticia['Data'] = agora.strftime("%d/%m/%Y")
            dicNoticia['Hora'] =  "00:00"
            dicNoticia['PalavrasChave'] = list(set([ele1 for ele1 in palavras_chave if ele1.lower() in noticiasel[i].text]))
            dicNoticia['Titulo'] = noticiasel[i].find_all('a')[0].text.replace("\n", "")
            dicNoticia['Link'] = noticiasel[i].find_all('a')[0].get('href')
            noticias.append(dicNoticia)
        else:
            continue
    
    
    dfloop = pd.DataFrame(noticias)
    if len(dfloop) !=0:
        dfloop = dfloop[pd.to_datetime(dfloop['Hora'])> (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")]
        dfloop =dfloop[dfloop['Data'] == agora.strftime("%d/%m/%Y")]
    return dfloop



########################################### setting ########################################## 
#Johann:
#file_path = 'C:/Users/Johann/OneDrive/Área de Trabalho/Agreg_News/index.html'  
#Bia:
file_path = '/Users/bianogueira/Documents/Documentos - MacBook Air de Beatriz/RIO/academico/puc rio economia/ARX/webscraping/newsscraping/index.html'  # Replace with the path to your HTML file
palavras_chave = ['Preços', 'Preço', 'Tarifas', 'Tarifa', 'Gasolina', 'GLP', 'Botijão', 'Petrobras', ' PPI ', 'paridade', 'Reajuste', 'Brent', 'Cotação Safra', ' IPI ', 'ICMS', ' PIS ', 'Cofins', 'Impostos', 'Imposto', 'Reforma', 'Reformas', 'Governo', 'Congresso', 'Câmara', 'Senado', 'Deputados', 'Governador', ' PEC ', 'Centrão', 'Fiscal', 'Superávit', 'Déficit', 'Dívida', 'Arrecadação', ' Lira ', 'Pacheco', ' Lula ', 'Haddad', 'Fazenda', 'Ministério', 'STF', 'Judiciário', 'Banco Central', 'Autonomia', 'Taxa de juros', 'Juros', 'Política Monetária', 'Copom', 'Campos Neto', 'Galípolo', 'Ceron', 'Guilherme Mello', 'BNDES', 'TJLP', 'Mercadante', 'Nelson Barbosa', 'Inflação']
dicsiteandfunction = {
    'Valor':[scrape_valor,'https://valor.globo.com/ultimas-noticias/'],
    'Estadao':[scrape_estadao, 'https://acesso.estadao.com.br/login/?r=https://www.estadao.com.br/ultimas/'],
    'oGlobo':[scrape_valor, 'https://oglobo.globo.com/ultimas-noticias/'],
    'Folha': [scrape_folha, 'https://www1.folha.uol.com.br/ultimas-noticias/'],
    'CNN': [scrape_cnnbr, 'https://www.cnnbrasil.com.br/ultimas-noticias/'],
    'G1': [scrape_valor, 'https://g1.globo.com/ultimas-noticias/'],
    'G1_flor': [scrape_valor, 'https://g1.globo.com/economia/blog/ana-flor/'],
    'G1_sadi': [scrape_valor, 'https://g1.globo.com/politica/blog/andreia-sadi/'],
    'G1_camarotti': [scrape_valor, 'https://g1.globo.com/politica/blog/gerson-camarotti/'],
    'G1_duailibi': [scrape_valor, 'https://g1.globo.com/politica/blog/julia-duailibi/'],
    'G1_nrey': [scrape_valor, 'https://g1.globo.com/politica/blog/natuza-nery/'],
    'G1_cruz': [scrape_valor, 'https://g1.globo.com/politica/blog/valdo-cruz/'],
    'Poder360':[scrape_poder360, 'https://www.poder360.com.br/'],
    'Metropoles':[scrape_metropoles, 'https://www.metropoles.com/ultimas-noticias'],
    'Veja':[scrape_veja, 'https://veja.abril.com.br/ultimas-noticias'],
    'ANEEL':[scrape_aneel, 'https://www.gov.br/aneel/pt-br/assuntos/noticias'],
    'nAgricolas':[scrape_nAgro, 'https://www.noticiasagricolas.com.br/noticias/'],
    'AntagonistaBr':[scrape_antagonista, 'https://oantagonista.uol.com.br/brasil/'],
    'AntagonistaInt':[scrape_antagonista, 'https://oantagonista.uol.com.br/economia/'],
    'Cepea':[scrape_cepea, 'https://www.cepea.esalq.usp.br/br/categoria/diarias-de-mercado.aspx'],
    'Scot':[scrape_scot, 'https://www.scotconsultoria.com.br/noticias/todas-noticias/'],
    'CanalEnergia':[scrape_ce,'https://www.canalenergia.com.br/noticias' ], 
    'Agrolink': [scrape_agrolink, 'https://www.agrolink.com.br/noticias/lista'],
    'CanalRuralpg1':[scrape_crural, 'https://www.canalrural.com.br/noticias/'],
    'CanalRuralpg2':[scrape_crural, 'https://www.canalrural.com.br/noticias/page/2/'],
    'DiarioTransporte': [scrape_dtransporte, 'https://diariodotransporte.com.br/?s=']
    }




########################################### main ########################################## 


while True:
    if (datetime.datetime.today().strftime("%M")[1] == '0' or datetime.datetime.today().strftime("%M")[1] == '5'):
        
        
        
        if (datetime.datetime.today().strftime("%H:%M") == '09:00' or datetime.datetime.today().strftime("%H:%M") == '13:00' or datetime.datetime.today().strftime("%H:%M") == '17:00'):
            
            dicsiteandfunctionloop = dicsiteandfunction
        else:
            dicsiteandfunctionloop = {k: dicsiteandfunction[k] for k in dicsiteandfunction.keys() & {'Valor', 'Estadao', 'oGlobo', 'Folha', 'CNN', 'G1', 'G1_flor', 'G1_sadi', 'G1_camarotti', 'G1_duailibi', 'G1_nrey', 'G1_cruz', 'Poder360', 'Metropoles', 'Veja'}}
        
        
        
        dfnoticias = pd.DataFrame(columns = ['Data', 'Hora', 'PalavrasChave', 'Titulo', 'Link'])



        #comecar cronometro
        start = time.time()
        #estabelecer variaveis de hora
        agora = datetime.datetime.today()
        horariolimite = (agora - datetime.timedelta(minutes = 30)) .strftime("%H:%M")
        datalimite = agora.strftime("%d/%m/%Y")
        header = '\n<html>\n<head>\n<meta name="viewport" content="width=device-width, initial-scale=1">\n<style>\ntable {  border-collapse: collapse;  border-spacing: 0;  width: 100%;  border: 1px solid #ddd;}\nth, td {  text-align: left;  padding: 16px;}\ntr:nth-child(even) {  background-color: #f2f2f2;}\n</style>\n</head>\n<body>\n\n<h2>Agregador de Notícias - ARX Macro</h2>\n<p> Última Atualização: '+agora.strftime("%H:%M")+'</p>'

        #iniciar o browser
        options = Options()
        options.add_argument("--headless")
        options.page_load_strategy = "eager"
        driver = webdriver.Chrome(options = options)
        driver.set_window_size(1440, 900)
        
        
        #iniciar loop por dicionario de sites e funcoes
        for key in dicsiteandfunctionloop:
            print(key)
            if (key == "Estadao" or key == "Folha" or key == "ANEEL" or key == 'DiarioTransporte' or key == 'Veja' or key == 'Scot'):
                #fechar driver antigo
                driver.quit()
                #iniciar novo com estrategia de pagina diferente
                options.page_load_strategy = "none"
                driver = webdriver.Chrome(options = options)
                driver.set_window_size(1440, 900)
                #rodar a funcao
                try:
                    (func, url) = dicsiteandfunction[key]
                except (ElementClickInterceptedException, TimeoutException, IndexError) as e:
                    print("O site ", key, " não funcionou. Erro: " + traceback.print_exc() + "\n Passando para o próximo.")
                    continue
                dfloop = func(driver, url, palavras_chave,agora)
                dfnoticias = pd.concat([dfnoticias,dfloop], axis = 0, ignore_index = True)
                #voltar para a estrategia de pagina anterior
                driver.quit()
                options.page_load_strategy = "eager"
                driver = webdriver.Chrome(options = options)
                driver.set_window_size(1440, 900)
                
            else:
                try:
                    (func, url) = dicsiteandfunction[key]
                except (ElementClickInterceptedException, TimeoutException, IndexError) as e:
                    print("O site ", key, " não funcionou. Erro: " + traceback.print_exc() + "\n Passando para o próximo.")
                    continue
                dfloop = func(driver, url, palavras_chave,agora)
                dfnoticias = pd.concat([dfnoticias,dfloop], axis = 0, ignore_index = True)
        



        html_df = dfnoticias.copy()
        # Convert the link column to clickable links
        html_df['Link'] = html_df['Link'].apply(lambda x: f'<a href="{x}">{x}</a>')
        # Convert the DataFrame to HTML table
        html = html_df.to_html(index=False, escape=False,classes='table table-stripped')
        html = header + html


        display(html)

        
        #to_clickable_html(dfnoticias.sort_values('Hora', ascending = False, inplace = True), file_path, header)
        
        #webbrowser.open('file://'+ file_path, autoraise= False )
        

        end = time.time()
        print((end-start)/60)
        driver.quit()
        
        time.sleep(60)
    else:
        time.sleep(30)
        


